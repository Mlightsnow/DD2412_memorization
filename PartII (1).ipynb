{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c1ad612-437c-4064-b35b-d20bd3dc23ef"
      },
      "source": [
        "# Part II - Layer Criticality\n",
        "\n",
        "In this part of the assignment, we will focus on reproducing an intriguing phenomenon of deep networks, known as *layer criticality*, first reported by [Zhang et al. (2019)](https://openreview.net/forum?id=ryg1P4Sh2E).\n",
        "\n",
        "The authors explore whether every layer of a discriminative deep network contributes equally to the performance of the learned classifier. Strikingly, it was observed that for some layers, the value of the corresponding parameters can be reset to initialization without severly affecting performance of the resulting model.\n",
        "\n",
        "In practice, starting from a trained network and the initialization checkpoint that was used for training, you will reproduce the main result of Zhang et al., for a pre-trained ResNet18 which was produced using the code from Part I of this assignment.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Let $\\theta^t = (\\theta_1^t, \\ldots, \\theta_L^t)$ denote the parameters of a $L$-layer network at epoch $t$, with $\\theta_\\ell^t$ representing the parameters of the $\\ell$-th layer. Let $t = T$ denote the final epoch of training. Consider now $\\theta^0 = \\theta^t_{|t = 0}$ &mdash; corresponding to the parameters of the same network at the specific initialization that was used for training.\n",
        "\n",
        "Let $\\mathcal{L}(\\theta^t, \\mathcal{D}) := \\mathbb{E}_{B_i \\sim \\mathcal{D}}{[}\\mathcal{L}(B_i, \\theta^t){]}$ denote the average loss of the network with parameters $\\theta^t$ over mini-batches $B_i$ sampled from a finite dataset $\\mathcal{D} = \\{(\\mathbf{x}_n, y_n) \\}_{n=1}^N$.\n",
        "\n",
        "To study the criticality of each layer $\\ell = 1, \\ldots, L$ for performance of the network, the parameter $\\theta^T = (\\theta_1^T, \\ldots, \\theta_\\ell^T, \\ldots, \\theta_L^T)$ is modified by setting $\\theta_\\ell^T = \\theta_\\ell^t$, for $t < T$.\n",
        "\n",
        "Then, the relative drop in performance is computed as:\n",
        "$$ drop = \\big(\\mathcal{L}(\\theta^T) - \\mathcal{L}(\\theta^T_{|\\theta_\\ell^T = \\theta_\\ell^t}) \\big) \\big/ \\mathcal{L}(\\theta^T),$$\n",
        "where the dependency of $\\mathcal{L}$ on $\\mathcal{D}$ has been omitted to simplify notation.\n",
        "\n",
        "Given a list of checkpoint epochs $\\{t_1 = 0, \\ldots, t_s\\}$, the methodology proposed by Zhang et al. consists in iterating through the layers of a trained network $(t = T)$ and compute the drop in performance when a layer $\\ell$ is reset to its value at initialization, keeping all other layers fixed at their trained state.\n",
        "\n",
        "In this assigment, we will consider two choices of $\\mathcal{L}$, namely the cross-entropy loss and the 0/1 loss (accuracy). Finally, we will evaluate both losses on the train and test split of CIFAR-10.\n",
        "\n",
        "## Getting started\n",
        "\n",
        "In order to solve this assignment, you will need to [download the checkpoints](https://kth-my.sharepoint.com/:f:/g/personal/mgamba_ug_kth_se/EveGFRgj1zFAnoTtNVcbzecB37DsSi9AU8FeAGCx9R918g?e=VlfAUj) of a pretrained ResNet18 following the model definition of Part I. As previsouly noted, such model is a thinner version of ResNet v1 ([He et al., 2015](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html)), with fewer parameters to limit the computational requirements for this exercise. While the results reproduced in this assignment generalize to other datasets, we will use CIFAR-10 for simplicity ([Krizhevsky and Hinton, 2009](https://www.cs.toronto.edu/~kriz/cifar.html)).\n",
        "\n",
        "Note: it is a good idea to verify the integrity of the downloaded zip archive by matching its `sha256` hash with the one provided at the download link. Afterwards, extract all files from the archive and take note of the path where they are stored."
      ],
      "id": "0c1ad612-437c-4064-b35b-d20bd3dc23ef"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install --upgrade \"jax[cuda111]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmy0mig4RHrl",
        "outputId": "b0994c08-1528-4b6f-9344-e2482ef5f7ca"
      },
      "id": "hmy0mig4RHrl",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'flax=0.3.4=pypi_0': Expected end or semicolon (after name and no valid version specifier)\n",
            "    flax=0.3.4=pypi_0\n",
            "        ^ (from line 1 of requirements.txt)\n",
            "Hint: = is not a valid operator. Did you mean == ?\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\n",
            "Requirement already satisfied: jax[cuda111] in /usr/local/lib/python3.10/dist-packages (0.4.33)\n",
            "Collecting jax[cuda111]\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "\u001b[33mWARNING: jax 0.4.35 does not provide the extra 'cuda111'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting jaxlib<=0.4.35,>=0.4.34 (from jax[cuda111])\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: ml-dtypes>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from jax[cuda111]) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax[cuda111]) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda111]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax[cuda111]) (1.13.1)\n",
            "Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (87.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "Successfully installed jax-0.4.35 jaxlib-0.4.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c 'import jax; import torch; print(jax.devices()); print(torch.cuda.is_available())'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx5HzFL9RKaH",
        "outputId": "cedbbf19-72cd-47f6-9248-bfacd3832c0a"
      },
      "id": "Rx5HzFL9RKaH",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CpuDevice(id=0)]\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "326c0476-7c42-4401-bae1-01052e83ebd7"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "from flax.core import freeze, unfreeze, FrozenDict\n",
        "from flax import traverse_util\n",
        "import optax\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "import PIL.Image\n",
        "\n",
        "from typing import Any, Callable, Sequence, Tuple, Dict\n",
        "from functools import partial\n",
        "import random\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, Markdown, display\n",
        "import seaborn as sns"
      ],
      "id": "326c0476-7c42-4401-bae1-01052e83ebd7"
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./checkpoints\n",
        "!mkdir -p ./data"
      ],
      "metadata": {
        "id": "22_xJ-EGSZWs"
      },
      "id": "22_xJ-EGSZWs",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aecf2130-89ae-44a7-98df-6a8c31a31592"
      },
      "outputs": [],
      "source": [
        "os.environ['JAX_PLATFORM_NAME'] = 'gpu'\n",
        "# If running the notebook fails with OOM errors, please consider uncommenting the following line.\n",
        "# os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
        "# If the notebook is failing at the very beginning with OOM error, uncomment the following line,\n",
        "# and tune the amout of preallocated memory according to your system.\n",
        "# os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.80'\n",
        "\"\"\"\n",
        "==============================\n",
        "TODO: Configuration required.\n",
        "==============================\n",
        "Edit below to set your path to CIFAR-10, as well as the target directory for\n",
        "loading checkpoints, and the path to the pre-trained model `resnet18_99.pickle`.\n",
        "The pre-trained model is provided as part of this assignment, together with\n",
        "checkpoints of several training epochs and initialization.\n",
        "\n",
        "We will assume the naming convention of models trained using the code from part\n",
        "I of this assignment. For example, `resnet18_EPOCH.pickle` will denote a\n",
        "resnet18 model checkpoint taken at training epoch EPOCH.\n",
        "\"\"\"\n",
        "data_dir = \"./data\" # path to cifar-10-batches-py\n",
        "checkpoints_dir = \"./checkpoints\"\n",
        "converged_model_fname = \"./checkpoints/resnet18_55.pickle\" # path to converged model checkpoint\n",
        "assert os.path.exists(converged_model_fname),\\\n",
        "    \"The path you provided does not exists!\""
      ],
      "id": "aecf2130-89ae-44a7-98df-6a8c31a31592"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "62c2ddc6-367d-4985-915d-7a0f8277ac4a"
      },
      "outputs": [],
      "source": [
        "# Useful hyperparameters\n",
        "seed = 42\n",
        "enable_batch_norm = False\n",
        "batch_size = 128\n",
        "\n",
        "cifar10_mean = (0.4919, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2023, 0.1994, 0.2010)\n",
        "num_classes = 10\n",
        "\n",
        "\"\"\"\n",
        "==============================\n",
        "TODO: Configuration required.\n",
        "==============================\n",
        "Set the list of checkpoint epochs that will be used for the layer\n",
        "reinitialization experiments.\n",
        "The epochs should be picked by inspecting the checkpoint filenames of the\n",
        "provided pre-trained model.\n",
        "\n",
        "Keep in mind that training epochs are zero-indexed, and that a model\n",
        "initialization is saved using the special keyword 'init'.\n",
        "\n",
        "Your list of epochs should at least include checkpoints \"init\", and \"0\".\n",
        "\"\"\"\n",
        "epochs = [\"init\", \"0\", \"10\", \"20\", \"30\", \"40\", \"50\", \"55\"]\n",
        "\n",
        "# dictionary of epoch: checkpoint filename\n",
        "checkpoint_fnames = {\n",
        "    str(epoch): os.path.join(checkpoints_dir, f\"resnet18_{epoch}.pickle\") for epoch in epochs }"
      ],
      "id": "62c2ddc6-367d-4985-915d-7a0f8277ac4a"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7dd4df89-20fc-440f-b37f-6b07a037751a"
      },
      "outputs": [],
      "source": [
        "# Data loading utilities\n",
        "def numpy_collate(batch):\n",
        "    \"\"\"Collate batch into a single numpy.ndarray\"\"\"\n",
        "    if isinstance(batch[0], np.ndarray):\n",
        "        return np.stack(batch)\n",
        "    elif isinstance(batch[0], (tuple,list)):\n",
        "        transposed = zip(*batch)\n",
        "        return [numpy_collate(samples) for samples in transposed]\n",
        "    else:\n",
        "        return np.array(batch)\n",
        "\n",
        "\n",
        "class NumpyLoader(torch.utils.data.DataLoader):\n",
        "    \"\"\"Numpy dataloader subclassing pytorch's data loader\"\"\"\n",
        "    def __init__(self, dataset, batch_size=1,\n",
        "                  shuffle=False, sampler=None,\n",
        "                  batch_sampler=None, num_workers=0,\n",
        "                  pin_memory=False, drop_last=False,\n",
        "                  timeout=0, worker_init_fn=None):\n",
        "        super(self.__class__, self).__init__(dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            sampler=sampler,\n",
        "            batch_sampler=batch_sampler,\n",
        "            num_workers=num_workers,\n",
        "            collate_fn=numpy_collate,\n",
        "            pin_memory=pin_memory,\n",
        "            drop_last=drop_last,\n",
        "            timeout=timeout,\n",
        "            worker_init_fn=worker_init_fn)\n",
        "\n",
        "\n",
        "# Transforms\n",
        "class ArrayNormalize(torch.nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, arr: np.ndarray) -> np.ndarray:\n",
        "        assert isinstance(arr, np.ndarray), \"Input should be ndarray, got {}.\".format(type(arr))\n",
        "        assert arr.ndim >= 3, \"Expected array to be image of size (*, H, W, C). Got {}.\".format(arr.shape)\n",
        "\n",
        "        dtype = arr.dtype\n",
        "        mean = np.asarray(self.mean, dtype=dtype)\n",
        "        std = np.asarray(self.std, dtype=dtype)\n",
        "        if (std == 0).any():\n",
        "            raise ValueError(\"std evaluated to zero after conversion to {}\".format(dtype))\n",
        "        if mean.ndim == 1:\n",
        "            mean = mean.reshape(1, 1, -1)\n",
        "        if std.ndim == 1:\n",
        "            std = std.reshape(1, 1, -1)\n",
        "        arr -= mean\n",
        "        arr /= std\n",
        "        return arr\n",
        "\n",
        "\n",
        "class ToArray(torch.nn.Module):\n",
        "    dtype = np.float32\n",
        "\n",
        "    def __call__(self, x):\n",
        "        assert isinstance(x, PIL.Image.Image)\n",
        "        x = np.asarray(x, dtype=self.dtype)\n",
        "        x /= 255.0\n",
        "        return x\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    ToArray(),\n",
        "    ArrayNormalize(cifar10_mean, cifar10_std)])"
      ],
      "id": "7dd4df89-20fc-440f-b37f-6b07a037751a"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "53612af3-49b5-4fa3-9b89-94c304e7e702",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db8f11c-e61f-4871-8886-c1f635c2b3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 36.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# load CIFAR-10\n",
        "cifar10_train = CIFAR10(data_dir, download=True, transform=transform, train=True)\n",
        "cifar10_test = CIFAR10(data_dir, download=True, transform=transform, train=False)"
      ],
      "id": "53612af3-49b5-4fa3-9b89-94c304e7e702"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1ce80a49-aaa8-43e4-820f-f2f113d590c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f154aa80-a704-4f5a-ce6a-19f0461c3c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# set seed for reproducibilty\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "# instantiate data loaders\n",
        "train_loader = NumpyLoader(cifar10_train, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
        "test_loader = NumpyLoader(cifar10_test, batch_size=2*batch_size, num_workers=0)"
      ],
      "id": "1ce80a49-aaa8-43e4-820f-f2f113d590c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1SN3e9vqkmz"
      },
      "source": [
        "## ResNet model definition"
      ],
      "id": "D1SN3e9vqkmz"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "00885277-c17a-466e-b807-b2c74f8c03f4"
      },
      "outputs": [],
      "source": [
        "ModuleDef = Any\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    \"\"\"ResNet basic block\"\"\"\n",
        "    filters: int\n",
        "    conv: ModuleDef\n",
        "    norm: ModuleDef\n",
        "    act: Callable\n",
        "    strides: Tuple[int,int] = (1,1)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        residual = x\n",
        "        y = self.conv(self.filters, (3,3), self.strides)(x)\n",
        "        if self.norm is not None:\n",
        "            y = self.norm()(y)\n",
        "        y = self.act(y)\n",
        "        y = self.conv(self.filters, (3,3))(y)\n",
        "        if self.norm is not None:\n",
        "            y = self.norm(scale_init=nn.initializers.zeros)(y)\n",
        "\n",
        "        if residual.shape != y.shape:\n",
        "            residual = self.conv(self.filters, (1,1), self.strides, name='conv_proj')(residual)\n",
        "            if self.norm is not None:\n",
        "                residual = self.norm(name='norm_proj')(residual)\n",
        "\n",
        "        return self.act(residual + y)\n",
        "\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    \"\"\"Bottleneck residual block\"\"\"\n",
        "    filters: int\n",
        "    conv: ModuleDef\n",
        "    norm: ModuleDef\n",
        "    act: Callable\n",
        "    strides: Tuple[int, int] = (1, 1)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        residual = x\n",
        "        y = self.conv(self.filters, (1,1))(x)\n",
        "        if self.norm is not None:\n",
        "            y = self.norm()(y)\n",
        "        y = self.act(y)\n",
        "        y = self.conv(self.filters, (3, 3), self.strides)(y)\n",
        "        if self.norm is not None:\n",
        "            y = self.norm()(y)\n",
        "        y = self.act(y)\n",
        "        y = self.conv(self.filters * 4, (1,1))(y)\n",
        "        if self.norm is not None:\n",
        "            y = self.norm(scale_init=nn.initializers.zeros)(y)\n",
        "\n",
        "        if residual.shape != y.shape:\n",
        "            residual = self.conv(self.filters * 4, (1, 1), self.strides, name='conv_proj')(residual)\n",
        "            if self.norm is not None:\n",
        "                residual = self.norm(name='norm_proj')(residual)\n",
        "\n",
        "        return self.act(residual + y)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"ResNet v1\"\"\"\n",
        "    stage_sizes: Sequence[int]\n",
        "    block_cls: ModuleDef\n",
        "    num_classes: int = 10 # adapted to CIFAR-10\n",
        "    num_filters: int = 16 # reduced number of filters to decrease training time\n",
        "    dtype: Any = jnp.float32\n",
        "    act: Callable = nn.relu\n",
        "\n",
        "    def setup(self, enable_batch_norm=False):\n",
        "        self.enable_batch_norm = enable_batch_norm\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, train: bool = True):\n",
        "        conv = partial(nn.Conv, use_bias = not self.enable_batch_norm, dtype = self.dtype)\n",
        "        if self.enable_batch_norm:\n",
        "            norm = partial(nn.BatchNorm, use_running_average=not train, momentum=0.9, epsilon=1e-5, dtype=self.dtype)\n",
        "        else:\n",
        "            norm = None\n",
        "\n",
        "        x = conv(self.num_filters, (3,3), (1,1),\n",
        "                 padding='SAME', name='conv_init')(x)\n",
        "        if self.enable_batch_norm:\n",
        "            x = norm(name='bn_init')(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.max_pool(x, (2, 2), strides=(2, 2), padding='SAME')\n",
        "        for i, block_size in enumerate(self.stage_sizes):\n",
        "            for j in range(block_size):\n",
        "                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n",
        "                x = self.block_cls(self.num_filters * 2 ** i,\n",
        "                                   strides=strides,\n",
        "                                   conv=conv,\n",
        "                                   norm=norm,\n",
        "                                   act=self.act)(x)\n",
        "        x = jnp.mean(x, axis=(1, 2))\n",
        "        x = nn.Dense(self.num_classes, dtype=self.dtype)(x)\n",
        "        x = jnp.asarray(x, self.dtype)\n",
        "        return x\n",
        "\n",
        "ResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2], block_cls=ResNetBlock)"
      ],
      "id": "00885277-c17a-466e-b807-b2c74f8c03f4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-C_GKhVqhyy"
      },
      "source": [
        "## Evaluation utilities"
      ],
      "id": "9-C_GKhVqhyy"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5f5c602b-7bd0-443e-9ae7-5de166b43579"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(logits, labels):\n",
        "    \"\"\"Given the model predictions @logits and ground-truth @labels compute\n",
        "       loss and accuracy.\n",
        "    ==============================\n",
        "    TODO: Implementation required.\n",
        "    ==============================\n",
        "    Copy here your implementation of @compute_metrics from part I.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'loss': loss\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(params, batch):\n",
        "    \"\"\"Evaluate ResNet18 on batch using params.\n",
        "    \"\"\"\n",
        "    logits = ResNet18().apply({'params': params}, batch['image'], train=False)\n",
        "    return compute_metrics(logits=logits, labels=batch['label'])\n",
        "\n",
        "\n",
        "def eval_model(epoch, params, data_loader):\n",
        "    \"\"\"Evaluate model using data loader\n",
        "    \"\"\"\n",
        "    batch_metrics = []\n",
        "    for input, target in data_loader:\n",
        "        batch = {\n",
        "            'image': input,\n",
        "            'label': target,\n",
        "        }\n",
        "\n",
        "        metrics = eval_step(params, batch)\n",
        "        batch_metrics.append(metrics)\n",
        "\n",
        "    batch_metrics_np = jax.device_get(batch_metrics)\n",
        "    epoch_metrics_np = {\n",
        "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
        "        for k in batch_metrics_np[0]\n",
        "    }\n",
        "    split = \"train\" if data_loader.dataset.train else \"test\"\n",
        "    print(\"epoch: {}, {} loss: {}, {} accuracy: {}\".format(\n",
        "            epoch, split, epoch_metrics_np['loss'], split, epoch_metrics_np['accuracy']))\n",
        "    return epoch_metrics_np"
      ],
      "id": "5f5c602b-7bd0-443e-9ae7-5de166b43579"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOJgLkGSqfNL"
      },
      "source": [
        "## Utilities for working with parameter dictionaries"
      ],
      "id": "hOJgLkGSqfNL"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2xU8wtf4q4c8"
      },
      "outputs": [],
      "source": [
        "def load_state_dict(checkpoint_fname):\n",
        "    \"\"\"Load state dictionary from file.\n",
        "    \"\"\"\n",
        "    state_dict = torch.load(checkpoint_fname)\n",
        "    return state_dict\n",
        "\n",
        "\n",
        "def load_checkpoint(rng, checkpoint_fname):\n",
        "    \"\"\"Restore saved checkpoint\n",
        "    \"\"\"\n",
        "    net = ResNet18(num_classes=num_classes)\n",
        "    params = net.init(rng, jnp.ones((1, 32, 32, 3)))['params']\n",
        "    state_dict = load_state_dict(checkpoint_fname)\n",
        "\n",
        "    params = unfreeze(params)\n",
        "    params = state_dict[\"state\"]\n",
        "    params = freeze(params)\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def flatten_dict(d: Dict) -> Dict:\n",
        "    \"\"\"Traverse a nested dictionary @d and return a flattened version of it.\n",
        "    \"\"\"\n",
        "    return {'/'.join(k): v for k, v in traverse_util.flatten_dict(d).items()}\n",
        "\n",
        "\n",
        "def unflatten_dict(d: Dict) -> Dict:\n",
        "    \"\"\"Unflatten parameter dictionary @d back to a nested dictionary\n",
        "    \"\"\"\n",
        "    return traverse_util.unflatten_dict({tuple(k.split('/')): v for k, v in d.items()})\n",
        "\n",
        "\n",
        "def print_flat_params_dict(d: Dict) -> None:\n",
        "    \"\"\"Print flattened dictionary, together with the shape of all tensors it\n",
        "       contains\n",
        "    \"\"\"\n",
        "    print(jax.tree_map(jnp.shape, d))\n",
        "\n",
        "\n",
        "def layers(params: FrozenDict) -> str:\n",
        "    \"\"\"Yield each layer of a parameter dictionary\n",
        "    \"\"\"\n",
        "    def is_leaf(prefix, xs):\n",
        "        return 'kernel' in list(xs.keys())\n",
        "\n",
        "    layer_ids = [ '/'.join(k) for k, _ in traverse_util.flatten_dict(unfreeze(params), is_leaf=is_leaf).items() ]\n",
        "    for layer in layer_ids:\n",
        "        yield layer\n",
        "\n",
        "\n",
        "def num_layers(params: FrozenDict) -> int:\n",
        "    \"\"\"Return the number of layers in a parameter dictionary\n",
        "    \"\"\"\n",
        "    return len(list(layers(params)))"
      ],
      "id": "2xU8wtf4q4c8"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Oq08N0lYq1xq"
      },
      "outputs": [],
      "source": [
        "def performance_drop(eval_metrics, metrics_converged):\n",
        "    \"\"\"Compute the performance drop of @eval_metrics relative to @metrics_converged\n",
        "\n",
        "       ==============================\n",
        "       TODO: Implementation required.\n",
        "       ==============================\n",
        "       For each metric in @eval_metrics, compute the relative decrease in performance\n",
        "       from the corresponding metric in @metrics_converged.\n",
        "\n",
        "       @return Dict: a dictionary {'accuracy': acc_drop, 'loss': loss_incr}\n",
        "               where acc_drop is the relative drop in accuracy computed by this function\n",
        "               and loss_incr is the relative loss increase.\n",
        "    \"\"\"\n",
        "\n",
        "    accuracy_drop = (metrics_converged['accuracy'] - eval_metrics['accuracy']) / metrics_converged['accuracy']\n",
        "    loss_increase = (eval_metrics['loss'] - metrics_converged['loss']) / metrics_converged['loss']\n",
        "    perf_drop = {\n",
        "        'accuracy': accuracy_drop,\n",
        "        'loss': loss_increase\n",
        "    }\n",
        "\n",
        "    return perf_drop"
      ],
      "id": "Oq08N0lYq1xq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqVYVRlJKWo"
      },
      "source": [
        "test the `performance_drop` function"
      ],
      "id": "1CqVYVRlJKWo"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cl-XT9C6JK1O"
      },
      "outputs": [],
      "source": [
        "eval_metrics = {'accuracy': 60,\n",
        "                'loss' : 7.33}\n",
        "metrics_converged = {'accuracy': 99,\n",
        "                'loss' : 5.02}\n",
        "expected = {'accuracy': 0.3939393,\n",
        "            'loss': 0.4601593}\n",
        "output = performance_drop(eval_metrics, metrics_converged)\n",
        "for v1,v2 in zip(output.values(),\n",
        "                 expected.values()):\n",
        "    np.testing.assert_almost_equal(v1,v2,)"
      ],
      "id": "cl-XT9C6JK1O"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "POikuS8crB8Q"
      },
      "outputs": [],
      "source": [
        "def update_metrics(epoch, metrics, new_metrics, split):\n",
        "    \"\"\"Update the dictionary metrics[split] with the keys and values\n",
        "       from @new_metrics. Return the updated dictionary.\n",
        "    \"\"\"\n",
        "    for key in new_metrics:\n",
        "        try:\n",
        "            metrics[split][key][str(epoch)].append(new_metrics[key].item())\n",
        "        except KeyError:\n",
        "            metrics[split][key][str(epoch)] = [new_metrics[key].item()]\n",
        "    return metrics"
      ],
      "id": "POikuS8crB8Q"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jCLjza9Aq_20"
      },
      "outputs": [],
      "source": [
        "def reinit_layer(params: FrozenDict, params_init: FrozenDict, layer_id: str) -> FrozenDict:\n",
        "    \"\"\"\n",
        "    ==============================\n",
        "    TODO: Implementation required.\n",
        "    ==============================\n",
        "    Set @params at key @layer_id with the corresponding value from @params_init\n",
        "\n",
        "    Hint: both weight and bias (if present) of layer @layer_id should be reinitialized\n",
        "    Hint: the FrozenDict params needs to be unfrozen and flattened for you to operate upon\n",
        "    Hint: the returned dictionary should be a FrozenDict\n",
        "\n",
        "    @params: parameter dictionary of a trained model\n",
        "    @params_init: parameter dictionary of the same model, at initialization\n",
        "    @layer_id: dictionary key denoting the layer whose parameters are to be reinitialized.\n",
        "               The key should be chosen by inspecting the flattened dict @params.\n",
        "\n",
        "    @return modified_params\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Unfreeze and flatten the parameter dictionaries\n",
        "    params_flat = params.unfreeze()\n",
        "    params_init_flat = params_init.unfreeze()\n",
        "\n",
        "    # Step 2: Get all keys that start with layer_id\n",
        "    layer_keys = [k for k in params_flat.keys() if k.startswith(layer_id)]\n",
        "\n",
        "    # Step 3: Update the parameters for all matching keys\n",
        "    for key in layer_keys:\n",
        "        params_flat[key] = params_init_flat[key]\n",
        "\n",
        "    # Step 4: Return the modified parameters as a FrozenDict\n",
        "    modified_params = FrozenDict(params_flat)\n",
        "\n",
        "    return modified_params"
      ],
      "id": "jCLjza9Aq_20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M401G20UJBHd"
      },
      "source": [
        "test the `reinit_layer` function:"
      ],
      "id": "M401G20UJBHd"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "b41aHu8pJAvp"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(seed)\n",
        "test_params = freeze(\n",
        "    {\"test_layer_0\":{\n",
        "        \"bias\": jnp.zeros(4),\n",
        "        \"kernel\": jnp.zeros((1,4)),\n",
        "        },\n",
        "     \"test_layer_1\": {\n",
        "        \"Conv_0\": {\n",
        "            \"bias\": jnp.zeros(6),\n",
        "            \"kernel\": jnp.zeros((4,6)),\n",
        "        },\n",
        "        \"Conv_1\": {\n",
        "            \"bias\": jnp.zeros(8),\n",
        "            \"kernel\": jnp.zeros((6,8)),\n",
        "        }\n",
        "        },\n",
        "     \"test_layer_2\":{\n",
        "        \"kernel\": jnp.zeros((8,4)),\n",
        "        },\n",
        "    })\n",
        "test_params_init = freeze(\n",
        "    {\"test_layer_0\":{\n",
        "        \"bias\": jnp.ones(4),\n",
        "        \"kernel\": jnp.ones((1,4)),\n",
        "        },\n",
        "     \"test_layer_1\": {\n",
        "        \"Conv_0\": {\n",
        "            \"bias\": jnp.ones(6),\n",
        "            \"kernel\": jnp.ones((4,6)),\n",
        "        },\n",
        "        \"Conv_1\": {\n",
        "            \"bias\": jnp.ones(8),\n",
        "            \"kernel\": jnp.ones((6,8)),\n",
        "        }\n",
        "        },\n",
        "     \"test_layer_2\":{\n",
        "        \"kernel\": jnp.ones((8,4)),\n",
        "        },\n",
        "    })\n",
        "\n",
        "test_params_flat = flatten_dict(test_params)\n",
        "test_params_init_flat = flatten_dict(test_params_init)\n",
        "layer_id = next(layers(test_params))\n",
        "returned = reinit_layer(test_params,\n",
        "                        test_params_init,\n",
        "                        layer_id)\n",
        "assert isinstance(returned,FrozenDict),\\\n",
        "    'the output of reinit_layer function should be of FrozenDict type'\n",
        "\n",
        "returned_flat = flatten_dict(returned)\n",
        "for layer in test_params_flat:\n",
        "    if layer_id in layer:\n",
        "        np.testing.assert_array_equal(\n",
        "            returned_flat[layer],\n",
        "            test_params_init_flat[layer])\n",
        "    else:\n",
        "        np.testing.assert_array_equal(\n",
        "            returned_flat[layer],\n",
        "            test_params_flat[layer])"
      ],
      "id": "b41aHu8pJAvp"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bf825bfc-c5a5-430c-bacf-cd6f7ca20be2"
      },
      "outputs": [],
      "source": [
        "def reinit_layers(epoch, metrics, metrics_converged, params, params_epoch, train_loader, test_loader):\n",
        "    \"\"\"\n",
        "    Loop through the layers of the @params dictionary. For each layer,\n",
        "    reinitialize the correponding entry in @params with that of @params_epoch\n",
        "\n",
        "    Use @train_loader and @test_loader to evaluate the model resulting from\n",
        "    modifying @params.\n",
        "\n",
        "    Compute the drop in performance of the modified model from the converged model\n",
        "    and store the result in @metrics using the key @epoch.\n",
        "    \"\"\"\n",
        "    # loop through layers\n",
        "    for l in layers(params):\n",
        "        print(\"Reinitializing layer {}\".format(l))\n",
        "        params_reinit = reinit_layer(params, params_epoch, l)\n",
        "\n",
        "        # eval network\n",
        "        eval_metrics = eval_model(epoch, params_reinit, test_loader)\n",
        "        train_metrics = eval_model(epoch, params_reinit, train_loader)\n",
        "        del params_reinit\n",
        "\n",
        "        # performance drop\n",
        "        perf_drop_test = performance_drop(eval_metrics, metrics_converged['test'])\n",
        "        perf_drop_train = performance_drop(train_metrics, metrics_converged['train'])\n",
        "\n",
        "        # update metrics dictionary\n",
        "        metrics = update_metrics(epoch, metrics, perf_drop_test, split='test')\n",
        "        metrics = update_metrics(epoch, metrics, perf_drop_train, split='train')\n",
        "\n",
        "    return metrics"
      ],
      "id": "bf825bfc-c5a5-430c-bacf-cd6f7ca20be2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv6LExOia1Hg"
      },
      "source": [
        "## main function"
      ],
      "id": "xv6LExOia1Hg"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "db45fb34-1dd8-4bf0-bce3-23f44e33554d"
      },
      "outputs": [],
      "source": [
        "def reinit_network(checkpoint_fnames, converged_model_fname, train_loader, test_loader):\n",
        "    \"\"\"For each layer L and checkpoint C, reinitialize layer L to the parameters\n",
        "       values at checkpoint C and compute the relative drop in validation\n",
        "       performance.\n",
        "    \"\"\"\n",
        "    # Load checkpoint of converged model\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    params = load_checkpoint(rng, converged_model_fname)\n",
        "\n",
        "    # Initialize results dictionary\n",
        "    metrics = {\n",
        "        'train': {\n",
        "            'loss' : {},\n",
        "            'accuracy': {},\n",
        "        },\n",
        "        'test': {\n",
        "            'loss': {},\n",
        "            'accuracy': {},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # dictionary of train/eval metrics for converged model\n",
        "    metrics_converged = {\n",
        "        'train' : { },\n",
        "        'test': { }\n",
        "    }\n",
        "\n",
        "    # compute performance of converged model\n",
        "    print(\"Computing performance of converged model\")\n",
        "    metrics_converged['train'].update(\n",
        "        eval_model(99, params, test_loader))\n",
        "    metrics_converged['test'].update(\n",
        "        eval_model(99, params, train_loader))\n",
        "\n",
        "    # iterate through checkpoint fnames\n",
        "    for epoch, checkpoint in checkpoint_fnames.items():\n",
        "        params_epoch = load_checkpoint(rng, checkpoint)\n",
        "        # iterate through layers\n",
        "        metrics = reinit_layers(\n",
        "            epoch, metrics, metrics_converged, params, params_epoch, train_loader, test_loader)\n",
        "        del params_epoch\n",
        "\n",
        "    return metrics"
      ],
      "id": "db45fb34-1dd8-4bf0-bce3-23f44e33554d"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "79d89c67-94e9-4cad-b019-7c293f3813b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f6704af-1763-4c34-c210-2b65cfad69f7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-f3a2ca771af1>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(checkpoint_fname)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing performance of converged model\n",
            "epoch: 99, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 99, train loss: 31.779863357543945, train accuracy: 0.0\n",
            "Reinitializing layer Dense_0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-f3a2ca771af1>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(checkpoint_fname)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: init, test loss: 7.780092716217041, test accuracy: 0.06660155951976776\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: init, train loss: 9.25697135925293, train accuracy: 0.0346754789352417\n",
            "Reinitializing layer ResNetBlock_0/Conv_0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-360027ac7b5f>:15: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  accuracy_drop = (metrics_converged['accuracy'] - eval_metrics['accuracy']) / metrics_converged['accuracy']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.781753540039062, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_0/Conv_1\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.779041290283203, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_1/Conv_0\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.78209686279297, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_1/Conv_1\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.782011032104492, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_2/Conv_0\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.775089263916016, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_2/Conv_1\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.78289794921875, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_2/conv_proj\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.782020568847656, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_3/Conv_0\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.775856018066406, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_3/Conv_1\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.776182174682617, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_4/Conv_0\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.78243637084961, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_4/Conv_1\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.773103713989258, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_4/conv_proj\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.777799606323242, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_5/Conv_0\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.775196075439453, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_5/Conv_1\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.77726936340332, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_6/Conv_0\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.777639389038086, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_6/Conv_1\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.7862548828125, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_6/conv_proj\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.777023315429688, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_7/Conv_0\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.77793312072754, train accuracy: 0.0\n",
            "Reinitializing layer ResNetBlock_7/Conv_1\n",
            "epoch: init, test loss: 21.509838104248047, test accuracy: 0.05927734449505806\n",
            "epoch: init, train loss: 31.780473709106445, train accuracy: 0.0\n",
            "Reinitializing layer conv_init\n",
            "epoch: init, test loss: 8.809141159057617, test accuracy: 0.09160156548023224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ca3e1d35b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1568, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-3755502f527e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinit_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_fnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverged_model_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-aeaeaf25cc49>\u001b[0m in \u001b[0;36mreinit_network\u001b[0;34m(checkpoint_fnames, converged_model_fname, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mparams_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# iterate through layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         metrics = reinit_layers(\n\u001b[0m\u001b[1;32m     40\u001b[0m             epoch, metrics, metrics_converged, params, params_epoch, train_loader, test_loader)\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mparams_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-8e1db85e1d36>\u001b[0m in \u001b[0;36mreinit_layers\u001b[0;34m(epoch, metrics, metrics_converged, params, params_epoch, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# eval network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0meval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_reinit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_reinit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mparams_reinit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7ca6c9abf14c>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(epoch, params, data_loader)\u001b[0m\n\u001b[1;32m     38\u001b[0m         }\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mbatch_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "metrics = reinit_network(checkpoint_fnames, converged_model_fname, train_loader, test_loader)"
      ],
      "id": "79d89c67-94e9-4cad-b019-7c293f3813b0"
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw6HIKydxbGo",
        "outputId": "c651a33f-fede-4e83-9da6-1c4dc3a7e984"
      },
      "id": "uw6HIKydxbGo",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote remove origin\n",
        "!git remote add origin git@github.com:Mlightsnow/DD2412_memorization.git"
      ],
      "metadata": {
        "id": "IfAX8UJ5yAq3"
      },
      "id": "IfAX8UJ5yAq3",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9TJsh6gylrx",
        "outputId": "e4116f66-7a63-428f-9e47-2374f719565e"
      },
      "id": "f9TJsh6gylrx",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBizGyx2yt65",
        "outputId": "6dafa316-511d-41f9-ed37-65d16add85e8"
      },
      "id": "eBizGyx2yt65",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# github.com:22 SSH-2.0-babeld-4909cb0f2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push -u origin master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlL9pBgI00-9",
        "outputId": "98ed4ecc-3c76-4652-da3a-e213144ddc77"
      },
      "id": "FlL9pBgI00-9",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 49, done.\n",
            "Counting objects:   2% (1/49)\rCounting objects:   4% (2/49)\rCounting objects:   6% (3/49)\rCounting objects:   8% (4/49)\rCounting objects:  10% (5/49)\rCounting objects:  12% (6/49)\rCounting objects:  14% (7/49)\rCounting objects:  16% (8/49)\rCounting objects:  18% (9/49)\rCounting objects:  20% (10/49)\rCounting objects:  22% (11/49)\rCounting objects:  24% (12/49)\rCounting objects:  26% (13/49)\rCounting objects:  28% (14/49)\rCounting objects:  30% (15/49)\rCounting objects:  32% (16/49)\rCounting objects:  34% (17/49)\rCounting objects:  36% (18/49)\rCounting objects:  38% (19/49)\rCounting objects:  40% (20/49)\rCounting objects:  42% (21/49)\rCounting objects:  44% (22/49)\rCounting objects:  46% (23/49)\rCounting objects:  48% (24/49)\rCounting objects:  51% (25/49)\rCounting objects:  53% (26/49)\rCounting objects:  55% (27/49)\rCounting objects:  57% (28/49)\rCounting objects:  59% (29/49)\rCounting objects:  61% (30/49)\rCounting objects:  63% (31/49)\rCounting objects:  65% (32/49)\rCounting objects:  67% (33/49)\rCounting objects:  69% (34/49)\rCounting objects:  71% (35/49)\rCounting objects:  73% (36/49)\rCounting objects:  75% (37/49)\rCounting objects:  77% (38/49)\rCounting objects:  79% (39/49)\rCounting objects:  81% (40/49)\rCounting objects:  83% (41/49)\rCounting objects:  85% (42/49)\rCounting objects:  87% (43/49)\rCounting objects:  89% (44/49)\rCounting objects:  91% (45/49)\rCounting objects:  93% (46/49)\rCounting objects:  95% (47/49)\rCounting objects:  97% (48/49)\rCounting objects: 100% (49/49)\rCounting objects: 100% (49/49), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (42/42), done.\n",
            "Writing objects: 100% (49/49), 357.21 MiB | 5.43 MiB/s, done.\n",
            "Total 49 (delta 5), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (5/5), done.\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: Trace: 7ab9fcc1a4356110927c649095112767b7157e034bb8a3edc82f3dd2441f3b71\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: File data/cifar-10-python.tar.gz is 162.60 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To github.com:Mlightsnow/DD2412_memorization.git\n",
            " \u001b[31m! [remote rejected]\u001b[m master -> master (pre-receive hook declined)\n",
            "\u001b[31merror: failed to push some refs to 'github.com:Mlightsnow/DD2412_memorization.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen -t ed25519 -C \"yhanmowsnoo@gmail.com\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHRzgkLU10e0",
        "outputId": "260dd7a2-e875-4d73-d3e5-0d502b7b3357"
      },
      "id": "nHRzgkLU10e0",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private ed25519 key pair.\n",
            "Enter file in which to save the key (/root/.ssh/id_ed25519): \n",
            "Created directory '/root/.ssh'.\n",
            "Enter passphrase (empty for no passphrase): \n",
            "Enter same passphrase again: \n",
            "Your identification has been saved in /root/.ssh/id_ed25519\n",
            "Your public key has been saved in /root/.ssh/id_ed25519.pub\n",
            "The key fingerprint is:\n",
            "SHA256:23UdTeGNXpQUNpa+nsu0qFLABDwEJG9GVaSj8vmeMyg yhanmowsnoo@gmail.com\n",
            "The key's randomart image is:\n",
            "+--[ED25519 256]--+\n",
            "|   ..+=++o    .BB|\n",
            "|    +  o..    +*+|\n",
            "|     + o+     oo+|\n",
            "|    o . .o   . +.|\n",
            "|   . .  S . . o o|\n",
            "|    o .  o o . . |\n",
            "|     o. . o   ...|\n",
            "|   E ..o..    +o.|\n",
            "|    . .+o .... +.|\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /root/.ssh/id_ed25519.pub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kYBQcF_2JTq",
        "outputId": "4c9f7512-a397-4c14-c714-540290e75d2f"
      },
      "id": "4kYBQcF_2JTq",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIPBKw2zyJ+4Xmbx7BxY+MxVlXc/B2/DtySptpyR19LXl yhanmowsnoo@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "489ce586-2400-437e-8a13-2bc8b3223ed1"
      },
      "source": [
        "## Criticality heatmaps\n",
        "\n",
        "Finally, we visualize the performance drop of reinitializing each layer at a specified epoch as a heatmap. By observing the plots below, answer the following.\n",
        "\n",
        "1. Which layers are most critical for performance?\n",
        "\n",
        "typically the early convolutional layers and final classification layers tend to be most critical in CNNs like ResNet18.\n",
        "2. Can you see stronger impact on cross-entropy loss or 0/1 loss (accuracy)?\n",
        "\n",
        "Generally, cross-entropy loss tends to be more sensitive to parameter changes than accuracy, so it may show a stronger impact.\n",
        "3. If several layers can be reinitialized without considerable impact on performance, can you speculate on their role for training?\n",
        "\n",
        "Layers that can be reinitialized without much impact may be:\n",
        "Redundant or overparameterized\n",
        "Primarily helping with optimization during training rather than being critical for final performance\n",
        "Acting as feature transformations that can be easily relearned\n",
        "Please note that layers are sorted in the order they are stored as a `FrozenDict`, rather than in the way they appear in the network. This is reflected in the criticality heatmaps."
      ],
      "id": "489ce586-2400-437e-8a13-2bc8b3223ed1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30effb60-d5ec-45ce-8b14-17bb7f00853f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "0e60338f-a71f-49ad-efcc-0b94034ef6d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'metrics' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b944d4a494bf>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mplot_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrit_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mplot_heatmaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
          ]
        }
      ],
      "source": [
        "# plot the results\n",
        "def get_layer_names():\n",
        "    params = load_checkpoint(jax.random.PRNGKey(seed), converged_model_fname)\n",
        "    names = list(layers(params))\n",
        "    return names\n",
        "\n",
        "def to_matrix(results_dict):\n",
        "    \"\"\"Turn dictionary of lists into numpy matrix\"\"\"\n",
        "    matrix = np.asarray([ results_dict[key] for key in results_dict ], dtype=np.float64)\n",
        "    return matrix\n",
        "\n",
        "def plot_heatmap(crit_matrix, fname, cmap=None):\n",
        "    \"\"\"Plot heatmap given matrix. Save the figure to @fname\"\"\"\n",
        "    save_name = fname + '.png'\n",
        "    xlabels = get_layer_names()\n",
        "    ax = sns.heatmap(crit_matrix, xticklabels=xlabels, yticklabels=epochs, linewidth=0.5, cmap=cmap)\n",
        "    plt.savefig(save_name, bbox_inches=\"tight\", dpi=200)\n",
        "    plt.clf()\n",
        "    display(Markdown(f\"### {fname}\"))\n",
        "    display(Image(save_name))\n",
        "\n",
        "def plot_heatmaps(metrics, epochs, cmap=None):\n",
        "    \"\"\"Visualize each entry of @metrics as heatmap\"\"\"\n",
        "    def is_leaf(prefix, xs):\n",
        "        return epochs[0] in list(xs.keys())\n",
        "\n",
        "    flattened_metrics = traverse_util.flatten_dict(metrics, is_leaf=is_leaf)\n",
        "    for mname, m in flattened_metrics.items():\n",
        "        fname = '_'.join(mname)\n",
        "        crit_matrix = to_matrix(m)\n",
        "        plot_heatmap(crit_matrix, fname, cmap)\n",
        "\n",
        "plot_heatmaps(metrics, epochs)"
      ],
      "id": "30effb60-d5ec-45ce-8b14-17bb7f00853f"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hoO-Y-Nw0zEY"
      },
      "id": "hoO-Y-Nw0zEY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a050deba-cd76-44a5-b20a-ef9744279a8d"
      },
      "source": [
        "## Distance from initalization\n",
        "\n",
        "Can criticality be merely interpreted as the amount of non-zero gradient updates received by a layer, i.e. how much its parameters have changed from initialization? One way to explore this hypothesis is to compute a correlation coefficient between the distance of each parameter $\\theta_i^T$ from the corresponding parameter $\\theta_i^0$ at initialization, and the criticality scores stored in `metrics`.\n",
        "\n",
        "First, you are going to implement functions to compute the L2 distance of two parameters in the same weight space."
      ],
      "id": "a050deba-cd76-44a5-b20a-ef9744279a8d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uInTNKRdrHvk"
      },
      "outputs": [],
      "source": [
        "# compute l2 distance from initialization\n",
        "@jax.jit\n",
        "def distance_from_init_step(param1, param2):\n",
        "    \"\"\"Compute l2 distance of param1 from param2\n",
        "       ==============================\n",
        "       TODO: Implementation required.\n",
        "       ==============================\n",
        "       Hint: assume the parameter tensors are flattened\n",
        "    \"\"\"\n",
        "\n",
        "    diff = param1 - param2\n",
        "    l2_dist = jnp.sqrt(jnp.sum(diff**2))\n",
        "\n",
        "    return l2_dist"
      ],
      "id": "uInTNKRdrHvk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77zSfZ-4O4qi"
      },
      "source": [
        "test the `distance_from_init_step` function:"
      ],
      "id": "77zSfZ-4O4qi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4vE8rpHO465"
      },
      "outputs": [],
      "source": [
        "returned = distance_from_init_step(\n",
        "    test_params_flat['test_layer_0/kernel'],\n",
        "    test_params_init_flat['test_layer_0/kernel'])\n",
        "expected = 2.\n",
        "np.testing.assert_almost_equal(returned,\n",
        "                               expected)"
      ],
      "id": "h4vE8rpHO465"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8fPl2PGrKnU"
      },
      "outputs": [],
      "source": [
        "def distance_from_init(params: FrozenDict, params_init: FrozenDict, layer_id: str) -> float:\n",
        "    \"\"\"Compute L2 distance of @params from @params_init at @layer_id. This\n",
        "       function should ignore any bias parameter, and operate only on the\n",
        "       weight tensor.\n",
        "\n",
        "       ==============================\n",
        "       TODO: Implementation required.\n",
        "       ==============================\n",
        "\n",
        "       Hint: To operate on FrozenDicts you should perform analogous operations\n",
        "             as you did in reinit_layer\n",
        "       Hint: This function should call the JIT-optimized `distance_from_init_step`.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the weight tensor for the specified layer\n",
        "    layer_params = params[layer_id]\n",
        "    layer_params_init = params_init[layer_id]\n",
        "\n",
        "    # Check if 'kernel' exists in the layer parameters\n",
        "    if 'kernel' in layer_params:\n",
        "        # Compute L2 distance using the JIT-optimized function\n",
        "        l2_dist = distance_from_init_step(\n",
        "            layer_params['kernel'],\n",
        "            layer_params_init['kernel']\n",
        "        )\n",
        "    else:\n",
        "        # If 'kernel' doesn't exist, return 0 (no distance)\n",
        "        l2_dist = 0.0\n",
        "\n",
        "    return l2_dist"
      ],
      "id": "c8fPl2PGrKnU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPh0osFhO_ey"
      },
      "source": [
        "test the `distance_from_init` function:"
      ],
      "id": "sPh0osFhO_ey"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-Y5jPaAO_-e"
      },
      "outputs": [],
      "source": [
        "returned = distance_from_init(\n",
        "    test_params,\n",
        "    test_params_init,\n",
        "    \"test_layer_0\")\n",
        "expected = 2.0\n",
        "np.testing.assert_almost_equal(returned,\n",
        "                               expected)"
      ],
      "id": "_-Y5jPaAO_-e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMw0pYKnrTeW"
      },
      "outputs": [],
      "source": [
        "def reinit_layers(epoch, metrics, params, params_epoch):\n",
        "    \"\"\"\n",
        "    ==============================\n",
        "    TODO: Implementation required.\n",
        "    ==============================\n",
        "    Modify the `reinit_layers` function to compute L2 distance of each weight\n",
        "    tensor in @params from the corresponding tensor in @params_epoch.\n",
        "\n",
        "    Hint: to save GPU memory, after you are done with a modified parameter dictionary\n",
        "          you should mark it for garbage collection.\n",
        "    \"\"\"\n",
        "    # loop through layers\n",
        "    for l in layers(params):\n",
        "\n",
        "        # Compute L2 distance for the current layer\n",
        "        l2_dist = distance_from_init(params, params_epoch, l)\n",
        "\n",
        "        try:\n",
        "            metrics['l2_dist'][epoch].append(l2_dist)\n",
        "        except KeyError:\n",
        "            metrics['l2_dist'][epoch] = [l2_dist]\n",
        "\n",
        "    return metrics"
      ],
      "id": "RMw0pYKnrTeW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMrRoDVoscf0"
      },
      "source": [
        "test the `reinit_layers` function:"
      ],
      "id": "GMrRoDVoscf0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgsqTO1HscIE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "b3e25869-e899-46e6-e4cd-3185c0411f42"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'test_layer_1/Conv_0'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-13dd9a874cc2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m returned = reinit_layers(\"test\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                          {\"l2_dist\":\n\u001b[1;32m      3\u001b[0m                           {\"test\":\n\u001b[1;32m      4\u001b[0m                            \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            }},\n",
            "\u001b[0;32m<ipython-input-29-72f8e4107244>\u001b[0m in \u001b[0;36mreinit_layers\u001b[0;34m(epoch, metrics, params, params_epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Compute L2 distance for the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0ml2_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_from_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1e56dba2d514>\u001b[0m in \u001b[0;36mdistance_from_init\u001b[0;34m(params, params_init, layer_id)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Extract the weight tensor for the specified layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlayer_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlayer_params_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/core/frozen_dict.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mFrozenDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'test_layer_1/Conv_0'"
          ]
        }
      ],
      "source": [
        "returned = reinit_layers(\"test\",\n",
        "                         {\"l2_dist\":\n",
        "                          {\"test\":\n",
        "                           []\n",
        "                           }},\n",
        "                         test_params,\n",
        "                         test_params_init)\n",
        "expected = np.array([2.,\n",
        "                     4.8989791,\n",
        "                     6.9282031,\n",
        "                     5.6568541])\n",
        "np.testing.assert_array_almost_equal(returned[\"l2_dist\"][\"test\"],\n",
        "                              expected)"
      ],
      "id": "DgsqTO1HscIE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f819ec86-f100-405b-bed5-2c422cf0e654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "7dc972d4-a3b3-461c-fc5a-da234cb04744"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'metrics' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-4ce35f04eca2>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l2_dist'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_from_init_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_fnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverged_model_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0ml2_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'l2_dist'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l2_dist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
          ]
        }
      ],
      "source": [
        "def distance_from_init_network(checkpoint_fnames, converged_model_fname, metrics):\n",
        "    \"\"\"For each layer L and checkpoint C, compute the L2 distance of the trained\n",
        "       parameters from those at the checkpoint C. Store the results in metrics.\n",
        "    \"\"\"\n",
        "    assert 'l2_dist' in metrics,\\\n",
        "        \"metrics should contain 'l2_dist' key.\"\n",
        "    # Load checkpoint of converged model\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    params = load_checkpoint(rng, converged_model_fname)\n",
        "\n",
        "    # iterate through checkpoint fnames\n",
        "    for epoch, checkpoint in checkpoint_fnames.items():\n",
        "        params_epoch = load_checkpoint(rng, checkpoint)\n",
        "        # iterate through layers\n",
        "        metrics = reinit_layers(\n",
        "            epoch, metrics, params, params_epoch)\n",
        "        del params_epoch\n",
        "\n",
        "    return metrics\n",
        "metrics['l2_dist'] = {}\n",
        "metrics = distance_from_init_network(checkpoint_fnames, converged_model_fname, metrics)\n",
        "l2_dist = {'l2_dist' : metrics['l2_dist']}\n",
        "plot_heatmaps(l2_dist, epochs, cmap=sns.color_palette(\"Blues\"))"
      ],
      "id": "f819ec86-f100-405b-bed5-2c422cf0e654"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92fab30b-ea20-473e-8352-c8fea3fb770d"
      },
      "source": [
        "### Spearman Correlation\n",
        "\n",
        "To conclude this exercise, you will compute the [Spearman rank correlation coeffiecient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) between the observed L2 distance $\\|\\theta^T_i - \\theta^0_i\\|_2$ for each layer, and the criticality values in `metrics` (train/test accuracy drop and loss increase).\n",
        "\n",
        "In general, a correlation coefficient very close to $1$ denotes strong correlation, while a value close to $-1$ denotes anti-correlation, and finally values around $0$ express no strong correlation."
      ],
      "id": "92fab30b-ea20-473e-8352-c8fea3fb770d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81c51a8e-cad8-4cfb-b69a-829200b9d75d"
      },
      "outputs": [],
      "source": [
        "def spearman_rank(metrics, epoch):\n",
        "    \"\"\"Compute the Spearman order correlation between the observed\n",
        "       L2 distances at @epoch and each metric in @metrics at @epoch\n",
        "\n",
        "       ==============================\n",
        "       TODO: Implementation required.\n",
        "       ==============================\n",
        "\n",
        "       Hint: for computing the Spearman rank, use `spearmanr` from `scipy`.\n",
        "             For this assignment, we are not interested in p_values, as\n",
        "             they are unreliable given the size of our observations.\n",
        "    \"\"\"\n",
        "    from scipy.stats import spearmanr\n",
        "    # Extract the L2 distance and metric values for the specified epoch\n",
        "    l2_dist_values = metrics['l2_dist'][epoch]\n",
        "    metric_values = metrics[metric][epoch]\n",
        "\n",
        "    # Compute Spearman rank correlation\n",
        "    l2_dist_correlation, _ = spearmanr(l2_dist_values, metric_values)\n",
        "\n",
        "    return l2_dist_correlation"
      ],
      "id": "81c51a8e-cad8-4cfb-b69a-829200b9d75d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeRQyaQ6W8iS"
      },
      "source": [
        "test the `spearman_rank` function:"
      ],
      "id": "EeRQyaQ6W8iS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96bkPmUQVQoI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "528b61ed-a2bc-468c-80b9-85fea14ecfcf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'metric' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-f4d07fe9b7f5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m returned = spearman_rank({\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"init\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"l2_dist\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"init\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     },\n\u001b[1;32m      5\u001b[0m     'init')\n",
            "\u001b[0;32m<ipython-input-32-6d086d19dd67>\u001b[0m in \u001b[0;36mspearman_rank\u001b[0;34m(metrics, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Extract the L2 distance and metric values for the specified epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0ml2_dist_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l2_dist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmetric_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Compute Spearman rank correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'metric' is not defined"
          ]
        }
      ],
      "source": [
        "returned = spearman_rank({\n",
        "    \"test\": {\"init\":[3,3,2,1]},\n",
        "    \"l2_dist\": {\"init\": [1,2,3,4]}\n",
        "    },\n",
        "    'init')\n",
        "expected = -0.9486832\n",
        "np.testing.assert_almost_equal(returned['test'],\n",
        "                               expected)"
      ],
      "id": "96bkPmUQVQoI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDd3UnakXQia"
      },
      "source": [
        "\n",
        "What conclusions can you draw from the computed Spearman correlations?\n",
        "\n",
        "this analysis would reveal if there's a relationship between how much a layer's parameters changed during training (L2 distance from initialization) and its criticality. A strong positive correlation would suggest layers that change more are more critical, while weak correlation would indicate criticality is not simply related to parameter change magnitude."
      ],
      "id": "lDd3UnakXQia"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_exEVM9sVMch",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "76c2cd2f-d86b-4883-c681-e76f7b682f13"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'metrics' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-fd8d0c617b49>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml2_dist_correlation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspearman_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'init'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spearman correlations: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_dist_correlation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
          ]
        }
      ],
      "source": [
        "l2_dist_correlation = spearman_rank(metrics, 'init')\n",
        "print(\"Spearman correlations: {}\".format(l2_dist_correlation))"
      ],
      "id": "_exEVM9sVMch"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv_Th2pIS6-3"
      },
      "source": [
        "Finally, it is valuable for us to know how long did it take you to complete this practical?"
      ],
      "id": "pv_Th2pIS6-3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVQGWFpxS6-3"
      },
      "outputs": [],
      "source": [
        "# 1 hour"
      ],
      "id": "FVQGWFpxS6-3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d16dda0b-9679-441d-aeb9-75bffbcf6ff5"
      },
      "source": [
        "## Optional: can non-critical layers be pruned?\n",
        "\n",
        "**Note.** This task is only provided for fun in case the practical has piqued your interest. Therefore, it is not part of the mandatory assignment and will not lead to a bonus either.\n",
        "As a optional task, repeat the criticality experiments on a VGG-11 network [Simonyan and Zisserman, 2015](https://www.robots.ox.ac.uk/~vgg/research/very_deep/) trained on CIFAR-10, using the code from Part I.\n",
        "\n",
        "More in detail, to complete the task you should:\n",
        "1. Extend the code of Part I to define a VGG-11 network (called 'config A' in the paper).\n",
        "2. Train VGG-11 on CIFAR-10, using data augmentation, no batch normalization, and on clean labels (no label noise). Save one checkpoint at initialization and for the trained model.\n",
        "3. Repeat the layer criticality experiment on VGG-11. What pattern do you observe in the criticality of its layers?\n",
        "4. Now go back to the code of Part I, and instantiate a new VGG model, by removing all non-critical layers. For this purpose, you can deem a layer non-critical if test accuracy drops at most by 0.1 when the layer is reset to its value at initialization.\n",
        "5. Train the newly defined model, with data augmentation and no batch norm. Can you match the same performance that you obtained with VGG-11?"
      ],
      "id": "d16dda0b-9679-441d-aeb9-75bffbcf6ff5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d0ca219-aacc-4491-95c4-6451aecbd01a"
      },
      "source": [
        "## References\n",
        "* [Are All Layers Created Equal?](https://openreview.net/forum?id=ryg1P4Sh2E) - Zhang et al. ICML Workshop Deep Phenomena, 2019.\n",
        "* [Very Deep Convolutional Networks for Large-Scale Visual Recognition](https://www.robots.ox.ac.uk/~vgg/research/very_deep/) - Simonyan and Zisserman. ICRL 2015.\n",
        "* [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html) - He et al. ICCV 2015.\n",
        "* [Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/cifar.html) - Krizhevsky and Hinton. 2009."
      ],
      "id": "1d0ca219-aacc-4491-95c4-6451aecbd01a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5192440b-b96b-4d00-82cb-54330e627c1a"
      },
      "source": [
        "## Acknowledgements\n",
        "* The code for operating on parameter dictionaries is adapted from the official [Flax Model Surgery tutorial](https://flax.readthedocs.io/en/latest/howtos/model_surgery.html)."
      ],
      "id": "5192440b-b96b-4d00-82cb-54330e627c1a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c79b2061-4110-40ad-8743-aad0a0300407"
      },
      "source": [
        "## Changelog\n",
        "| Version \t| Contribution      \t| Author (Affiliation) \t                | Contact \t        |\n",
        "|---------\t|-------------------\t|-----------------------------------    |---------\t        |\n",
        "| 1.0     \t| First development \t| Matteo Gamba (KTH/EECS/RPL)       \t|  ![contact address](figs/contact.png \"Contact information\") \t|"
      ],
      "id": "c79b2061-4110-40ad-8743-aad0a0300407"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}